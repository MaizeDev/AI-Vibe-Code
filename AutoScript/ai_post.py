import datetime
import argparse
import subprocess
import json
import os
import re
import hashlib
import time
import requests
import openai
from pathlib import Path

# =========================
# é…ç½®éƒ¨åˆ†
# =========================
API_KEY = os.getenv("AI_POST_KEY", "sk-BIMlxWA1ksae6qASYOBDFlW1e4xByrALU9DHOHevCOyAeuyJ")
OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", "https://x666.me/v1")
GEMINI_BASE_URL = os.getenv("GEMINI_BASE_URL", "https://x666.me")

DEFAULT_MODEL = "gemini-2.5-pro-1m"
DEFAULT_GROUP = "level3"

# =========================
# å·¥å…·å‡½æ•°
# =========================

def extract_json_block(text: str) -> dict:
    """æå– JSONï¼Œå¿½ç•¥æœ«å°¾å¯èƒ½é™„åŠ çš„å®Œæˆæ ‡è®°"""
    text = text.strip()
    
    # ç§»é™¤å¯èƒ½å­˜åœ¨çš„å®Œæˆæ ‡è®°
    text = re.sub(r"<!--.*?-->", "", text, flags=re.DOTALL).strip()

    if "```" in text:
        match = re.search(r"```(?:json)?\s*(\{.*?\})\s*```", text, re.DOTALL)
        if match:
            text = match.group(1)

    if not text.startswith("{"):
        match = re.search(r"\{.*\}", text, re.DOTALL)
        if match:
            text = match.group(0)

    try:
        text = text.replace("ï¼Œ", ",").replace("ï¼š", ":").replace("â€œ", '"').replace("â€", '"')
        return json.loads(text)
    except json.JSONDecodeError:
        print(f"âš ï¸ JSON è§£æå¤±è´¥ï¼ŒåŸå§‹å†…å®¹æ‘˜è¦:\n{text[:200]}...\n")
        return {}

def make_slug(title: str) -> str:
    slug = title.strip()
    slug = re.sub(r"[^\w\u4e00-\u9fa5-]", "-", slug)
    slug = re.sub(r"-+", "-", slug).strip("-")
    if not slug:
        slug = hashlib.md5(title.encode("utf-8")).hexdigest()[:8]
    return slug

def generate_front_matter(title, date, tags=None, draft=False):
    tags = tags or []
    lines = [
        "---",
        f'title: "{title}"',
        f"date: {date.isoformat()}",
        f"draft: {str(draft).lower()}",
    ]
    if tags:
        lines.append("tags:")
        for t in tags:
            lines.append(f"  - {t}")
    lines.append("---")
    return "\n".join(lines) + "\n\n"

# =========================
# æ ¸å¿ƒï¼šå¤šæ¨¡å‹è°ƒç”¨è·¯ç”± (å« HTML è¿‡æ»¤ä¸æˆªæ–­æ£€æµ‹)
# =========================

def call_llm(client, model, group, messages, temperature=0.7, max_retries=3):
    """
    å¢å¼ºç‰ˆè°ƒç”¨æ¥å£ï¼š
    1. å±è”½ HTML é”™è¯¯åˆ·å±
    2. æ£€æµ‹ finish_reason å¹¶è¿½åŠ çŠ¶æ€æ ‡è®°
    """
    
    for attempt in range(max_retries):
        try:
            content = ""
            is_truncated = False
            finish_reason = "unknown"

            # ===== Gemini åˆ†æ”¯ =====
            if model.startswith("gemini-"):
                url = f"{GEMINI_BASE_URL}/v1beta/models/{model}:generateContent"
                headers = {
                    "Content-Type": "application/json",
                    "x-goog-api-key": API_KEY,
                    "Authorization": f"Bearer {API_KEY}"
                }
                
                gemini_contents = []
                for m in messages:
                    role = "model" if m["role"] == "assistant" else "user"
                    gemini_contents.append({
                        "role": role,
                        "parts": [{"text": m["content"]}]
                    })

                payload = {
                    "contents": gemini_contents,
                    "generationConfig": {
                        "temperature": temperature,
                        "maxOutputTokens": 8192
                    }
                }

                resp = requests.post(url, headers=headers, json=payload, timeout=60)
                resp.raise_for_status()
                data = resp.json()
                
                # Gemini è§£æ
                candidate = data["candidates"][0]
                content = candidate["content"]["parts"][0]["text"]
                finish_reason = candidate.get("finishReason", "UNKNOWN")
                
                # Gemini çš„æˆªæ–­æ ‡è®°é€šå¸¸æ˜¯ "MAX_TOKENS"
                if finish_reason == "MAX_TOKENS":
                    is_truncated = True

            # ===== OpenAI / Claude åˆ†æ”¯ =====
            else:
                response = client.chat.completions.create(
                    model=model,
                    messages=messages,
                    temperature=temperature,
                    extra_body={"group": group}
                )

                # å…¼å®¹æ€§æå–
                if hasattr(response, 'choices'):
                    choice = response.choices[0]
                    content = choice.message.content
                    finish_reason = choice.finish_reason
                elif isinstance(response, dict):
                    choice = response["choices"][0]
                    content = choice["message"]["content"]
                    finish_reason = choice.get("finish_reason")
                else:
                    choice = response["choices"][0]
                    content = choice["message"]["content"]
                    finish_reason = "unknown"

                # OpenAI çš„æˆªæ–­æ ‡è®°é€šå¸¸æ˜¯ "length"
                if finish_reason == "length":
                    is_truncated = True

            # ===== ç»“æœå¤„ç†ï¼šè¿½åŠ æ ‡è®° =====
            if is_truncated:
                print(f"âš ï¸  è­¦å‘Šï¼šå†…å®¹å¯èƒ½è¢«æˆªæ–­ (Reason: {finish_reason})")
                content += "\n\n<!-- âš ï¸ WARNING: CONTENT TRUNCATED (Max Tokens Reached) -->"
            else:
                # åªæœ‰é JSON è¯·æ±‚ï¼ˆä¹Ÿå°±æ˜¯æ­£æ–‡ç”Ÿæˆï¼‰æ‰åŠ è¿™ä¸ªæ ‡è®°ï¼Œé¿å…ç ´å JSON ç»“æ„
                # é€šè¿‡ç®€å•åˆ¤æ–­å†…å®¹æ˜¯å¦åƒ JSON æ¥å†³å®š
                if not content.strip().startswith("{"):
                    content += "\n\n<!-- âœ… AI GENERATION COMPLETE -->"

            return content

        except Exception as e:
            error_msg = str(e)
            # ğŸ›‘ æ ¸å¿ƒä¿®å¤ï¼šæ£€æµ‹æ˜¯å¦æ˜¯ HTML é”™è¯¯é¡µ
            if "<!DOCTYPE html>" in error_msg or "<html" in error_msg:
                clean_error = "Server returned HTML error page (Likely 502 Bad Gateway or 404 Not Found)"
            else:
                clean_error = error_msg[:200] + "..." if len(error_msg) > 200 else error_msg

            print(f"âŒ è°ƒç”¨å¤±è´¥ (ç¬¬ {attempt + 1}/{max_retries} æ¬¡): {clean_error}")
            
            if attempt < max_retries - 1:
                time.sleep(2)
            else:
                # æœ€åä¸€æ¬¡å¤±è´¥ï¼Œè¿”å›ç©ºæˆ–æŠ›å‡ºï¼Œè¿™é‡Œé€‰æ‹©æŠ›å‡ºè®©ä¸»ç¨‹åºåœæ­¢
                raise Exception(clean_error)

# =========================
# AI é€»è¾‘
# =========================

def generate_metadata(draft_text, client, model, group):
    print("ğŸ¤– æ­£åœ¨ç”Ÿæˆæ ‡é¢˜å’Œæ ‡ç­¾...")
    prompt = (
        "åŸºäºä»¥ä¸‹æ–‡ç« åˆç¨¿ï¼Œç”Ÿæˆä¸€ä¸ªå¸å¼•äººçš„ä¸­æ–‡æ ‡é¢˜å’Œ 3-5 ä¸ªç›¸å…³æ ‡ç­¾ã€‚\n"
        "è¯·ä¸¥æ ¼åªè¾“å‡º JSON æ ¼å¼ï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–è§£é‡Šæ–‡æœ¬ï¼š\n"
        "{\"title\": \"ä½ çš„æ ‡é¢˜\", \"tags\": [\"æ ‡ç­¾1\", \"æ ‡ç­¾2\"]}\n\n"
        f"æ–‡ç« å†…å®¹æ‘˜è¦ï¼š{draft_text[:2000]}"
    )

    try:
        content = call_llm(
            client, model, group,
            messages=[{"role": "user", "content": prompt}]
        )
        data = extract_json_block(content)
        title = data.get("title", "æœªå‘½åæ–‡ç« ")
        tags = data.get("tags", [])
        print(f"âœ… æ ‡é¢˜ï¼š{title}")
        return title, tags
    except Exception:
        print("âš ï¸ æ— æ³•ç”Ÿæˆå…ƒæ•°æ®ï¼Œä½¿ç”¨é»˜è®¤å€¼ã€‚")
        return "æœªå‘½åæ–‡ç« ", []

def optimize_with_ai(draft_text, title, client, model, group):
    print("âœï¸ æ­£åœ¨æ¶¦è‰²æ­£æ–‡ (æ™ºèƒ½åˆ†ææ–‡é£ä¸é€»è¾‘)...")
    
    system_prompt = (
        "ä½ æ˜¯ä¸€ä½**å…¨èƒ½å‹çš„èµ„æ·±ä¸»ç¼–**ï¼Œæ‹¥æœ‰æé«˜çš„æ–‡å­¦ç´ å…»å’Œç™¾ç§‘å…¨ä¹¦èˆ¬çš„çŸ¥è¯†å‚¨å¤‡ã€‚\n"
        "ä½ çš„æ ¸å¿ƒèƒ½åŠ›æ˜¯ï¼š**ç²¾å‡†æ•æ‰ä½œè€…æ„å›¾ï¼Œåœ¨ä¸æ”¹å˜åŸæ–‡é£æ ¼çš„å‰æä¸‹ï¼Œæå‡æ–‡ç« è´¨é‡ã€‚**\n\n"
        "è¯·ä¸¥æ ¼æ‰§è¡Œä»¥ä¸‹å¤„ç†æµç¨‹ï¼š\n\n"
        "1. ã€é£æ ¼è¯†åˆ«ä¸ä¿æŒ (æœ€é«˜ä¼˜å…ˆçº§)ã€‘ï¼š\n"
        "   - å…ˆåˆ†æåŸæ–‡çš„è¯­è°ƒï¼ˆæ˜¯çŠ€åˆ©åæ§½ã€æ„Ÿæ€§æ—¥è®°ã€ä¸¥è°¨æŠ€æœ¯ï¼Œè¿˜æ˜¯è½»æ¾éšç¬”ï¼Ÿï¼‰ã€‚\n"
        "   - **å¿…é¡»ä¿æŒè¿™ç§è¯­è°ƒ**ã€‚å¦‚æœä½œè€…åœ¨åæ§½ï¼Œè¯·è®©åæ§½æ›´ç²¾å‡†ï¼›å¦‚æœä½œè€…å¾ˆæ„Ÿæ€§ï¼Œè¯·ä¿ç•™æƒ…ç»ªæ³¢åŠ¨ã€‚\n"
        "   - âŒ ä¸¥ç¦å°†æ–‡ç« æ”¹å†™æˆâ€œAIå‘³â€åè¶³çš„å…¬æ–‡æˆ–æ•™ç§‘ä¹¦ï¼ˆæ‹’ç»æ»¥ç”¨â€œæ€»è€Œè¨€ä¹‹â€ã€â€œç»¼ä¸Šæ‰€è¿°â€ï¼‰ã€‚\n"
        "   - âœ… ä¿ç•™ä½œè€…çš„ä¸ªäººå£å¤´ç¦…æˆ–ç‹¬ç‰¹çš„ç»“å°¾æ–¹å¼ï¼ˆå¦‚â€œä¸æƒ³å†™äº†â€ã€â€œåƒé¥­å»â€ï¼‰ã€‚\n\n"
        "2. ã€äº‹å®ä¸é€»è¾‘â€œæ‰‹æœ¯â€ã€‘ï¼š\n"
        "   - **äº‹å®æ ¸æŸ¥**ï¼šæ£€æµ‹æ–‡ä¸­æ¶‰åŠçš„å†å²æ—¶é—´çº¿ã€ä¸“ä¸šæœ¯è¯­ã€æŠ€æœ¯å‚æ•°æˆ–åäººåè¨€ã€‚å‘ç°é”™è¯¯ï¼ˆå¦‚æ—¶é—´å€’ç½®ã€æ¦‚å¿µæ··æ·†ï¼‰å¿…é¡»æ— å£°ä¿®æ­£ã€‚\n"
        "   - **é€»è¾‘ç¼åˆ**ï¼šå¦‚æœæ–‡ä¸­å­˜åœ¨æ€ç»´è·³è·ƒï¼Œè¯·åœ¨ä¿ç•™åŸæ„åŸºç¡€ä¸Šï¼Œç”¨è‡ªç„¶çš„è¿‡æ¸¡å¥å°†å…¶ä¸²è”ï¼Œä½¿é€»è¾‘é“¾æ¡é—­ç¯ã€‚\n\n"
        "3. ã€é’ˆå¯¹æ€§ä¼˜åŒ–ç­–ç•¥ã€‘ï¼š\n"
        "   - **è‹¥æ˜¯è§‚ç‚¹/è¯„è®ºæ–‡**ï¼šå¼ºåŒ–è®ºæ®çš„åŠ›åº¦ï¼Œç¡®ä¿å› æœå…³ç³»æˆç«‹ã€‚\n"
        "   - **è‹¥æ˜¯å™äº‹/æ—¥è®°**ï¼šå¢å¼ºç”»é¢æ„Ÿå’Œä»£å…¥æ„Ÿï¼Œç†é¡ºæ—¶é—´çº¿ã€‚\n"
        "   - **è‹¥æ˜¯æŠ€æœ¯/è¯´æ˜æ–‡**ï¼šç¡®ä¿æ­¥éª¤å‡†ç¡®ï¼Œæœ¯è¯­è§„èŒƒï¼Œè¯­è¨€ç®€æ´ã€‚\n\n"
        "4. ã€è¾“å‡ºè§„èŒƒã€‘ï¼š\n"
        "   - è¾“å‡ºçº¯æ­£æ–‡ Markdownï¼Œä¸å¸¦æ ‡é¢˜ï¼Œä¸å¸¦ Front Matterã€‚\n"
        f"å‚è€ƒæ ‡é¢˜ï¼š{title}"
    )

    try:
        content = call_llm(
            client, model, group,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"è¯·æ¶¦è‰²è¿™ç¯‡è‰ç¨¿ï¼š\n\n{draft_text}"}
            ]
        )
        return content.strip()
    except Exception:
        print("âŒ æ­£æ–‡ç”Ÿæˆå¤±è´¥ï¼Œä¿ç•™åŸæ–‡ã€‚")
        return draft_text + "\n\n<!-- âŒ AI GENERATION FAILED -->"
# =========================
# ä¸»æµç¨‹
# =========================

def main():
    parser = argparse.ArgumentParser(description="AI åšå®¢è‡ªåŠ¨å‘å¸ƒå·¥å…·")
    parser.add_argument("draft_file", help="è‰ç¨¿æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--draft", action="store_true", help="æ ‡è®°ä¸ºè‰ç¨¿")
    parser.add_argument("--output_dir", default="content/posts", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--model", default=DEFAULT_MODEL, help="ä½¿ç”¨çš„æ¨¡å‹")
    parser.add_argument("--group", default=DEFAULT_GROUP, help="API åˆ†ç»„")
    parser.add_argument("--no-git", action="store_true", help="è·³è¿‡ Git æ“ä½œ")
    args = parser.parse_args()

    client = openai.OpenAI(
        api_key=API_KEY,
        base_url=OPENAI_BASE_URL
    )

    draft_path = Path(args.draft_file)
    if not draft_path.exists():
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {args.draft_file}")
        return

    with open(draft_path, "r", encoding="utf-8") as f:
        draft_text = f.read()

    # 1. ç”Ÿæˆå…ƒæ•°æ®
    title, tags = generate_metadata(draft_text, client, args.model, args.group)
    
    # 2. æ¶¦è‰²æ­£æ–‡
    body = optimize_with_ai(draft_text, title, client, args.model, args.group)

    # 3. ç»„åˆå†…å®¹
    now = datetime.datetime.now().astimezone()
    final_content = generate_front_matter(title, now, tags, args.draft) + body

    # 4. å†™å…¥æ–‡ä»¶
    filename = f"{now.date()}-{make_slug(title)}.md"
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    filepath = output_dir / filename
    
    with open(filepath, "w", encoding="utf-8") as f:
        f.write(final_content)
    
    print(f"ğŸ’¾ æ–‡ä»¶å·²ä¿å­˜è‡³ï¼š{filepath}")

    # 5. Git æ“ä½œ
    if not args.no_git:
        print("ğŸš€ æ­£åœ¨æ‰§è¡Œ Git æ¨é€...")
        try:
            subprocess.run(["git", "add", str(filepath)], check=True, capture_output=True)
            subprocess.run(["git", "commit", "-m", f"Add post: {title}"], check=True, capture_output=True)
            subprocess.run(["git", "push"], check=True, capture_output=True)
            print("âœ… Git æ¨é€æˆåŠŸï¼")
        except subprocess.CalledProcessError as e:
            print("âŒ Git æ“ä½œå¤±è´¥ï¼")
            # åŒæ ·é˜²æ­¢ git æŠ¥é”™åˆ·å±å¤ªé•¿ï¼Œåªæˆªå– stderr
            err_msg = e.stderr.decode('utf-8', errors='ignore')
            print(f"é”™è¯¯è¯¦æƒ…ï¼š{err_msg[:300]}...")

if __name__ == "__main__":
    main()